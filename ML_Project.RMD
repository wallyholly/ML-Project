---
title: "Weight Lifting Exercise Dataset"
output:
  html_document:
    keep_md: yes
---

## Download, read and preprocessing the data
```{r}
library(caret, quietly=TRUE)
# set URLs:
url_train <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
url_test <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'

# download training and test data 
#download.file(url = url_train, destfile = 'data_train.csv')
#download.file(url = url_test, destfile = 'data_test.csv')

# read in training and test data
pml_train <- read.csv(file = 'data_train.csv',
                      na.strings = c('NA','#DIV/0!',''))
pml_test <- read.csv(file = 'data_test.csv',
                     na.strings = c('NA','#DIV/0!',''))
```

## Split of training data
I split the training data in a training and a validation set in order to calculate the out of sample error: 
```{r}
# set seed in order to make the results reproducible
set.seed(4711)
# the new training set will contain 60% of the data out of the old training set 
inTrain <- createDataPartition(y=pml_train$classe,p=0.6,list=F)
pml_training<-pml_train[inTrain, ]
# the validation set will contain 40% of the data out of the old training set 
pml_train_val<-pml_train[-inTrain, ]
```

## Data preparation for machine learning algorithm
1. Not all of the variables in the datasets are useful for machine learning. A visual inspection of the data shows that the first seven variables are not useful. Therefore I delete them from both training sets.  
```{r}
pml_training <- pml_training[, -(1:7)]
pml_train_val <- pml_train_val[, -(1:7)]
```  
2. I will delete all variables with nearly zero variance because they will not contribute significantly to the machine learning success.    
```{r}
zero_var <- nearZeroVar(pml_training)
pml_training <- pml_training[, -zero_var]
pml_train_val <- pml_train_val[, -zero_var]
``` 
3. I will delete all variables with an excessive high share of missing-values because they will also not contribute significantly to the machine learning success.  
```{r}
mostlyNA <- sapply(pml_training, function(x) mean(is.na(x))) > 0.95
pml_training <- pml_training[, mostlyNA==F]
pml_train_val <- pml_train_val[, mostlyNA==F]
``` 
Notice that all steps are accomplished with the training set and the validation set. I use the training set in order to judge which variables should removed but the same variables are removed from the validation set as well.    

# Model fit and cross validation  
I decided to fit a random forest model for the data, because random forests and boosting algorithms are the most frequently used algorithms in machine learning. The model is fit based on the training set
```{r}
# Model fitting:
# method="rf"  -  random forest
# trControl method="cv", number=4  -  cross validation with 3 calculations
modFit <- train(pml_training$classe ~ ., method="rf", trControl=trainControl(method = "cv", number = 3), data=pml_training)
# show stats for final model
modFit$finalModel
```  
## Out of sample error  
In order to calculate the out of sample error, I will use the modFit model in order to make predictions for the validation set.
After that I build a confusion matrix in order to compare the predicted values to the in the validation set. 
```{r}
# predict pml_train_val
preds_val <- predict(modFit, newdata=pml_train_val)
confusionMatrix(preds_val, pml_train_val$classe)
```
# New model fit and prediction on the test set
## New model
In this step I fit a new random forest model with the whole training set (pml_train). Therefore I will do all steps above again without splitting the training set into a training and validation set.
```{r}
pml_train <- read.csv(file = 'data_train.csv',
                      na.strings = c('NA','#DIV/0!',''))
pml_test <- read.csv(file = 'data_test.csv',
                     na.strings = c('NA','#DIV/0!',''))
pml_train <- pml_train[, -(1:7)]
pml_test  <- pml_test[, -(1:7)]

zero_var <- nearZeroVar(pml_train)
pml_train <- pml_train[, -zero_var]
pml_test  <- pml_test[, -zero_var]

mostlyNA <- sapply(pml_train, function(x) mean(is.na(x))) > 0.95
pml_train <- pml_train[, mostlyNA==F]
pml_test <- pml_test[, mostlyNA==F]


# Model fitting:
# method="rf"  -  random forest
# trControl method="cv", number=3  -  cross validation with 3 calculations
modFit <- train(pml_train$classe ~ ., method="rf", trControl=trainControl(method = "cv", number = 3), data=pml_train)
# show stats for final model
modFit$finalModel
```  
## Prediction on test set
```{r}
preds_test <- predict(modFit, newdata=pml_test)
preds_test
```

